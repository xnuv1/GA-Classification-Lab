{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "41de1b0b-6d8c-4b64-a1bb-1a6239f9dd53",
   "metadata": {},
   "source": [
    "<img src=\"./assets/ga-logo.png\" style=\"float: left; margin: 20px; height: 55px\">\n",
    "\n",
    "# Lab: Classification: Predicting Left-Handedness from Psychological Factors\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3184f0d1-fe56-494d-baa6-20f69a2bd5d3",
   "metadata": {},
   "source": [
    "One way to define the data science process is as follows:\n",
    "\n",
    "1. Define the problem.\n",
    "2. Obtain the data.\n",
    "3. Explore the data.\n",
    "4. Model the data.\n",
    "5. Evaluate the model.\n",
    "\n",
    "We'll walk through a full data science problem in this lab. \n",
    "\n",
    "---\n",
    "## Step 1: Define The Problem.\n",
    "\n",
    "You're currently a data scientist working at a university. A professor of psychology is attempting to study the relationship between personalities and left-handedness. They have tasked you with gathering evidence so that they may publish.\n",
    "\n",
    "As a data scientist, you know that any real data science problem must be **specific** and **conclusively answerable**. For example:\n",
    "- Bad data science problem: \"What is the link between obesity and blood pressure?\"\n",
    "    - This is vague and is not conclusively answerable. That is, two people might look at the conclusion and one may say \"Sure, the problem has been answered!\" and the other may say \"The problem has not yet been answered.\"\n",
    "- Good data science problem: \"Does an association exist between obesity and blood pressure?\"\n",
    "    - This is more specific and is conclusively answerable. The problem specifically is asking for a \"Yes\" or \"No\" answer. Based on that, two independent people should both be able to say either \"Yes, the problem has been answered\" or \"No, the problem has not yet been answered.\"\n",
    "- Excellent data science problem: \"As obesity increases, how does blood pressure change?\"\n",
    "    - This is very specific and is conclusively answerable. The problem specifically seeks to understand the effect of one variable on the other.\n",
    "\n",
    "### In the context of the left-handedness and personality example, what are three specific and conclusively answerable problems that you could answer using data science? \n",
    "\n",
    "> You might find it helpful to check out the codebook in the repo for some inspiration."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f374561b-d2dd-4b6b-9390-0a4af34fac74",
   "metadata": {},
   "source": [
    "# Answer:\n",
    "\n",
    "Q1: Does an association exist between personality and left-handedness?\n",
    "\n",
    "Q2: Does an association exist between students favourting math classes over pottery classes and left-handedness?\n",
    "\n",
    "Q3: Does ans association exist beteen having short memory and left-handedness?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bea50d5-6a4d-4948-9fe5-42d6cd6c5afb",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 2: Obtain the data.\n",
    "\n",
    "### Read in the file titled \"data.csv\":\n",
    "> Hint: Despite being saved as a .csv file, you won't be able to simply `pd.read_csv()` this data!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eb2a1f5d-32fb-4415-87de-3455cee07a7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# library imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "%matplotlib inline\n",
    "np.set_printoptions(legacy='1.25')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "23ce3fe6-3e08-45e1-8426-f6f4fdfe5f03",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('data.csv', sep='\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb928620-0569-4ca1-b11d-efb77951d278",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Step 3: Explore the data.\n",
    "\n",
    "### Conduct background research:\n",
    "\n",
    "Domain knowledge is irreplaceable. Figuring out what information is relevant to a problem, or what data would be useful to gather, is a major part of any end-to-end data science project! For this lab, you'll be using a dataset that someone else has put together, rather than collecting the data yourself.\n",
    "\n",
    "Do some background research about personality and handedness. What features, if any, are likely to help you make good predictions? How well do you think you'll be able to model this? Write a few bullet points summarizing what you believe, and remember to cite external sources.\n",
    "\n",
    "You don't have to be exhaustive here. Do enough research to form an opinion, and then move on.\n",
    "\n",
    "> You'll be using the answers to Q1-Q44 for modeling; you can disregard other features, e.g. country, age, internet browser."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e593f731-6759-4de8-94c2-f5f84c9b3119",
   "metadata": {},
   "source": [
    "# Answer\n",
    "\n",
    "My research shows that handedness is weakly linked to personality and brain lateralization. Right-handers often display more analytical and structured trait, while left-handers tend to be more creative and open to new experiences. However, these differences are small, so predicting handedness from personality questions alone is likely to have low accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36a75853-6d9c-42c8-95ca-e0fdad19a17b",
   "metadata": {},
   "source": [
    "### Conduct exploratory data analysis on this dataset:\n",
    "\n",
    "If you haven't already, be sure to check out the codebook in the repo, as that will help in your EDA process.\n",
    "\n",
    "You might use this section to perform data cleaning if you find it to be necessary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c4461bb5-9dd2-46c2-b1cc-dca2d92e2290",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4184, 56)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "35a60b82-c1ce-43cc-9a71-f4f2ede3f459",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Q1</th>\n",
       "      <th>Q2</th>\n",
       "      <th>Q3</th>\n",
       "      <th>Q4</th>\n",
       "      <th>Q5</th>\n",
       "      <th>Q6</th>\n",
       "      <th>Q7</th>\n",
       "      <th>Q8</th>\n",
       "      <th>Q9</th>\n",
       "      <th>Q10</th>\n",
       "      <th>...</th>\n",
       "      <th>country</th>\n",
       "      <th>fromgoogle</th>\n",
       "      <th>engnat</th>\n",
       "      <th>age</th>\n",
       "      <th>education</th>\n",
       "      <th>gender</th>\n",
       "      <th>orientation</th>\n",
       "      <th>race</th>\n",
       "      <th>religion</th>\n",
       "      <th>hand</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>4184.000000</td>\n",
       "      <td>4184.000000</td>\n",
       "      <td>4184.000000</td>\n",
       "      <td>4184.000000</td>\n",
       "      <td>4184.000000</td>\n",
       "      <td>4184.000000</td>\n",
       "      <td>4184.000000</td>\n",
       "      <td>4184.000000</td>\n",
       "      <td>4184.000000</td>\n",
       "      <td>4184.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>4184</td>\n",
       "      <td>4184.000000</td>\n",
       "      <td>4184.000000</td>\n",
       "      <td>4184.000000</td>\n",
       "      <td>4184.000000</td>\n",
       "      <td>4184.000000</td>\n",
       "      <td>4184.000000</td>\n",
       "      <td>4184.000000</td>\n",
       "      <td>4184.000000</td>\n",
       "      <td>4184.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>94</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>US</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>2468</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>1.962715</td>\n",
       "      <td>3.829589</td>\n",
       "      <td>2.846558</td>\n",
       "      <td>3.186902</td>\n",
       "      <td>2.865440</td>\n",
       "      <td>3.672084</td>\n",
       "      <td>3.216539</td>\n",
       "      <td>3.184512</td>\n",
       "      <td>2.761233</td>\n",
       "      <td>3.522945</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.576243</td>\n",
       "      <td>1.239962</td>\n",
       "      <td>30.370698</td>\n",
       "      <td>2.317878</td>\n",
       "      <td>1.654398</td>\n",
       "      <td>1.833413</td>\n",
       "      <td>5.013623</td>\n",
       "      <td>2.394359</td>\n",
       "      <td>1.190966</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1.360291</td>\n",
       "      <td>1.551683</td>\n",
       "      <td>1.664804</td>\n",
       "      <td>1.476879</td>\n",
       "      <td>1.545798</td>\n",
       "      <td>1.342238</td>\n",
       "      <td>1.490733</td>\n",
       "      <td>1.387382</td>\n",
       "      <td>1.511805</td>\n",
       "      <td>1.242890</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.494212</td>\n",
       "      <td>0.440882</td>\n",
       "      <td>367.201726</td>\n",
       "      <td>0.874264</td>\n",
       "      <td>0.640915</td>\n",
       "      <td>1.303454</td>\n",
       "      <td>1.970996</td>\n",
       "      <td>2.184164</td>\n",
       "      <td>0.495357</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>13.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>18.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>21.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>3.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>27.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>23763.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>3.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>11 rows × 56 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Q1           Q2           Q3           Q4           Q5  \\\n",
       "count   4184.000000  4184.000000  4184.000000  4184.000000  4184.000000   \n",
       "unique          NaN          NaN          NaN          NaN          NaN   \n",
       "top             NaN          NaN          NaN          NaN          NaN   \n",
       "freq            NaN          NaN          NaN          NaN          NaN   \n",
       "mean       1.962715     3.829589     2.846558     3.186902     2.865440   \n",
       "std        1.360291     1.551683     1.664804     1.476879     1.545798   \n",
       "min        0.000000     0.000000     0.000000     0.000000     0.000000   \n",
       "25%        1.000000     3.000000     1.000000     2.000000     1.000000   \n",
       "50%        1.000000     5.000000     3.000000     3.000000     3.000000   \n",
       "75%        3.000000     5.000000     5.000000     5.000000     4.000000   \n",
       "max        5.000000     5.000000     5.000000     5.000000     5.000000   \n",
       "\n",
       "                 Q6           Q7           Q8           Q9          Q10  ...  \\\n",
       "count   4184.000000  4184.000000  4184.000000  4184.000000  4184.000000  ...   \n",
       "unique          NaN          NaN          NaN          NaN          NaN  ...   \n",
       "top             NaN          NaN          NaN          NaN          NaN  ...   \n",
       "freq            NaN          NaN          NaN          NaN          NaN  ...   \n",
       "mean       3.672084     3.216539     3.184512     2.761233     3.522945  ...   \n",
       "std        1.342238     1.490733     1.387382     1.511805     1.242890  ...   \n",
       "min        0.000000     0.000000     0.000000     0.000000     0.000000  ...   \n",
       "25%        3.000000     2.000000     2.000000     1.000000     3.000000  ...   \n",
       "50%        4.000000     3.000000     3.000000     3.000000     4.000000  ...   \n",
       "75%        5.000000     5.000000     4.000000     4.000000     5.000000  ...   \n",
       "max        5.000000     5.000000     5.000000     5.000000     5.000000  ...   \n",
       "\n",
       "        country   fromgoogle       engnat           age    education  \\\n",
       "count      4184  4184.000000  4184.000000   4184.000000  4184.000000   \n",
       "unique       94          NaN          NaN           NaN          NaN   \n",
       "top          US          NaN          NaN           NaN          NaN   \n",
       "freq       2468          NaN          NaN           NaN          NaN   \n",
       "mean        NaN     1.576243     1.239962     30.370698     2.317878   \n",
       "std         NaN     0.494212     0.440882    367.201726     0.874264   \n",
       "min         NaN     1.000000     0.000000     13.000000     0.000000   \n",
       "25%         NaN     1.000000     1.000000     18.000000     2.000000   \n",
       "50%         NaN     2.000000     1.000000     21.000000     2.000000   \n",
       "75%         NaN     2.000000     1.000000     27.000000     3.000000   \n",
       "max         NaN     2.000000     2.000000  23763.000000     4.000000   \n",
       "\n",
       "             gender  orientation         race     religion         hand  \n",
       "count   4184.000000  4184.000000  4184.000000  4184.000000  4184.000000  \n",
       "unique          NaN          NaN          NaN          NaN          NaN  \n",
       "top             NaN          NaN          NaN          NaN          NaN  \n",
       "freq            NaN          NaN          NaN          NaN          NaN  \n",
       "mean       1.654398     1.833413     5.013623     2.394359     1.190966  \n",
       "std        0.640915     1.303454     1.970996     2.184164     0.495357  \n",
       "min        0.000000     0.000000     0.000000     0.000000     0.000000  \n",
       "25%        1.000000     1.000000     5.000000     1.000000     1.000000  \n",
       "50%        2.000000     1.000000     6.000000     2.000000     1.000000  \n",
       "75%        2.000000     2.000000     6.000000     2.000000     1.000000  \n",
       "max        3.000000     5.000000     7.000000     7.000000     3.000000  \n",
       "\n",
       "[11 rows x 56 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.describe(include='all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e30c4aa8-3fd5-4f07-ae38-e70586dcc376",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Q1              int64\n",
       "Q2              int64\n",
       "Q3              int64\n",
       "Q4              int64\n",
       "Q5              int64\n",
       "Q6              int64\n",
       "Q7              int64\n",
       "Q8              int64\n",
       "Q9              int64\n",
       "Q10             int64\n",
       "Q11             int64\n",
       "Q12             int64\n",
       "Q13             int64\n",
       "Q14             int64\n",
       "Q15             int64\n",
       "Q16             int64\n",
       "Q17             int64\n",
       "Q18             int64\n",
       "Q19             int64\n",
       "Q20             int64\n",
       "Q21             int64\n",
       "Q22             int64\n",
       "Q23             int64\n",
       "Q24             int64\n",
       "Q25             int64\n",
       "Q26             int64\n",
       "Q27             int64\n",
       "Q28             int64\n",
       "Q29             int64\n",
       "Q30             int64\n",
       "Q31             int64\n",
       "Q32             int64\n",
       "Q33             int64\n",
       "Q34             int64\n",
       "Q35             int64\n",
       "Q36             int64\n",
       "Q37             int64\n",
       "Q38             int64\n",
       "Q39             int64\n",
       "Q40             int64\n",
       "Q41             int64\n",
       "Q42             int64\n",
       "Q43             int64\n",
       "Q44             int64\n",
       "introelapse     int64\n",
       "testelapse      int64\n",
       "country        object\n",
       "fromgoogle      int64\n",
       "engnat          int64\n",
       "age             int64\n",
       "education       int64\n",
       "gender          int64\n",
       "orientation     int64\n",
       "race            int64\n",
       "religion        int64\n",
       "hand            int64\n",
       "dtype: object"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8f95dc30-d1ac-4af3-93e4-c43189a5e0c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Q1             0\n",
       "Q2             0\n",
       "Q3             0\n",
       "Q4             0\n",
       "Q5             0\n",
       "Q6             0\n",
       "Q7             0\n",
       "Q8             0\n",
       "Q9             0\n",
       "Q10            0\n",
       "Q11            0\n",
       "Q12            0\n",
       "Q13            0\n",
       "Q14            0\n",
       "Q15            0\n",
       "Q16            0\n",
       "Q17            0\n",
       "Q18            0\n",
       "Q19            0\n",
       "Q20            0\n",
       "Q21            0\n",
       "Q22            0\n",
       "Q23            0\n",
       "Q24            0\n",
       "Q25            0\n",
       "Q26            0\n",
       "Q27            0\n",
       "Q28            0\n",
       "Q29            0\n",
       "Q30            0\n",
       "Q31            0\n",
       "Q32            0\n",
       "Q33            0\n",
       "Q34            0\n",
       "Q35            0\n",
       "Q36            0\n",
       "Q37            0\n",
       "Q38            0\n",
       "Q39            0\n",
       "Q40            0\n",
       "Q41            0\n",
       "Q42            0\n",
       "Q43            0\n",
       "Q44            0\n",
       "introelapse    0\n",
       "testelapse     0\n",
       "country        0\n",
       "fromgoogle     0\n",
       "engnat         0\n",
       "age            0\n",
       "education      0\n",
       "gender         0\n",
       "orientation    0\n",
       "race           0\n",
       "religion       0\n",
       "hand           0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "97123d4a-2dd4-4911-b9cb-aa9348bb290b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "hand\n",
       "1    3542\n",
       "2     452\n",
       "3     179\n",
       "0      11\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['hand'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e48958c0-5322-4ef4-a8dd-94dc681b21ff",
   "metadata": {},
   "source": [
    "### Short answer questions:\n",
    "\n",
    "In this lab you'll use K-nearest neighbors and logistic regression to model handedness based off of psychological factors. Answer the following related questions; your answers may be in bullet points.\n",
    "\n",
    "### 5. Suppose I wanted to use Q1 - Q44 to predict whether or not the person is left-handed. Would this be a classification or regression problem? Why?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "099bd14e-b121-41b5-88d0-e12c51cde6a3",
   "metadata": {},
   "source": [
    "Classification, because we will be predicting wether the person is left-handed or no. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a48dfc3c-42a1-46ca-b0ae-9e146737dbd6",
   "metadata": {},
   "source": [
    "### 6. We want to use $k$-nearest neighbors to predict whether or not a person is left-handed based on their responses to Q1 - Q44. Before doing that, however, you remember that it is often a good idea to standardize your variables. In general, why would we standardize our variables? Give an example of when we would standardize our variables."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff34ccd7-4bf8-4dc9-9526-e6c4ff0aeb6f",
   "metadata": {},
   "source": [
    "We standardize variables beacuse we want them in the same range.\n",
    "\n",
    "Ex.\n",
    "Scaling values that in hundreds and values in millions to comparable range."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "464f48e9-2a4a-45b4-aa97-03e0eca99848",
   "metadata": {},
   "source": [
    "### 7. Give an example of when we might not standardize our variables."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da9232a8-1f7e-4387-9e43-6b98bd0a7864",
   "metadata": {},
   "source": [
    "1- If they are in the same scale.\n",
    "\n",
    "2- if the model algorithem is not affected by the feature scale."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9507f57f-84e1-40e2-8b70-6e4ac89f8cf2",
   "metadata": {},
   "source": [
    "### 8. Based on your answers to 6 and 7, do you think we should standardize our predictor variables in this case? Why or why not?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bda5a29-e214-401c-9b67-915fa2b07853",
   "metadata": {},
   "source": [
    "Yes we should because KNN is distance based algorithem so feature scales has effect on it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c3e59df-9635-40f8-aee5-247faba3d597",
   "metadata": {},
   "source": [
    "### 9. We want to use $k$-nearest neighbors to predict whether or not a person is left-handed. What munging/cleaning do we need to do to our $y$ variable in order to explicitly answer this question? Do it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ebed3792-0022-4cb1-9875-39beff2dd141",
   "metadata": {},
   "outputs": [],
   "source": [
    "left_handed = pd.get_dummies(data['hand'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "85833961-0000-4a05-8243-88303885f0f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "left_handed.columns = ['Not_given', 'Right_handed', 'Left_handed', 'Both']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8f83565a-ed32-485a-beda-1b86b1a540c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "left_handed['Left_handed'] = left_handed['Left_handed'] + left_handed['Both']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5c391ad6-a6ec-49e9-ab37-9566caf0a3fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "left_handed.drop(columns = ['Not_given', 'Right_handed', 'Both'], inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "314b460a-140c-4f62-8b3b-10b512e5460f",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.concat((data,left_handed), axis =1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "491ccade-8ead-46de-9323-c448a16e14bd",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Step 4 & 5 Modeling: $k$-nearest neighbors\n",
    "\n",
    "### Train-test split your data:\n",
    "\n",
    "Your explanatory variables should be all Q columns. \n",
    "- (Optional : You may use stratify = 'y')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a08e2af2",
   "metadata": {},
   "source": [
    "I'll first create my `X` and `y` dataframes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "61e21aeb-2fe3-41a8-ad18-05bd8dff8308",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data.loc[:, 'Q1':'Q44']\n",
    "y = data['Left_handed']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "916ffcc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42, stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9021887e-6f11-4279-a1e9-c84111b1c5ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = StandardScaler()\n",
    "X_train_sc = sc.fit_transform(X_train)\n",
    "X_test_sc = sc.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "313fbf34",
   "metadata": {},
   "source": [
    "Being good data scientists, we know that we might not run just one type of model. We might run many different models and see which is best. \n",
    " - try k=3,5,15, and 25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "23046a02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k=3, train accuracy=0.872, test accuracy=0.812\n",
      "k=5, train accuracy=0.851, test accuracy=0.830\n",
      "k=15, train accuracy=0.850, test accuracy=0.848\n",
      "k=25, train accuracy=0.849, test accuracy=0.849\n"
     ]
    }
   ],
   "source": [
    "scores = []\n",
    "\n",
    "for k in [3, 5, 15, 25]:\n",
    "    knn = KNeighborsClassifier(n_neighbors=k)\n",
    "    knn.fit(X_train_sc, y_train)\n",
    "    \n",
    "    train_acc = knn.score(X_train_sc, y_train)\n",
    "    test_acc = knn.score(X_test_sc, y_test)\n",
    "    \n",
    "    scores.append((k, train_acc, test_acc))\n",
    "    print(f\"k={k}, train accuracy={train_acc:.3f}, test accuracy={test_acc:.3f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8ab5c3b-f92c-46db-999c-305712d30b80",
   "metadata": {},
   "source": [
    "### Evaluate your models:\n",
    "\n",
    "Evaluate each of your four models on the training and testing sets, and interpret the four scores. Are any of your models overfit or underfit? Do any of your models beat the baseline accuracy rate?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3770bbf0-8999-4b60-99a7-f58243dc2cd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline accuracy = 0.849\n"
     ]
    }
   ],
   "source": [
    "baseline_accuracy = y.value_counts(normalize=True).max()\n",
    "print(f\"Baseline accuracy = {baseline_accuracy:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "761b988f-f2fd-4162-a79f-8bbaa84b929d",
   "metadata": {},
   "source": [
    "- k=3 is slighlty overfit as the diffrence between training and test accuracy score is way higher then the rest.\n",
    "- k = 5 is slighlty overfit but way better then k=3 although not as good k=15 and k=25.\n",
    "- k=15 much better and almost in perfect generlization and same as the basline accuracy rate.\n",
    "- k=25 is in perfect generlization no under/over fit and samse as the basline accuracy rate."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
